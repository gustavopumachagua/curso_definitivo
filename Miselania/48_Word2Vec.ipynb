{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 47** | **Siguiente 49** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../README.md) | [‚è™](./47_TF-IDF.ipynb)| [‚è©](./49_Poisson_Distribution.ipynb)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **48. Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec: Explicaci√≥n Detallada**\n",
    "\n",
    "**Introducci√≥n:**\n",
    "\n",
    "Word2Vec es una t√©cnica popular en procesamiento de lenguaje natural (PLN) que se utiliza para representar palabras como vectores densos en un espacio n-dimensional. Fue desarrollada por Tomas Mikolov y su equipo en Google. La idea principal es asignar representaciones vectoriales a las palabras de manera que las palabras con significados similares est√©n cercanas entre s√≠ en el espacio vectorial.\n",
    "\n",
    "**Terminolog√≠a:**\n",
    "\n",
    "- **Word Embeddings:** Representaciones vectoriales de palabras en un espacio n-dimensional.\n",
    "  \n",
    "- **Skip-gram y CBOW (Continuous Bag of Words):** Dos arquitecturas de modelos Word2Vec. Skip-gram predice contextos (palabras circundantes) a partir de una palabra dada, mientras que CBOW predice una palabra dada su contexto.\n",
    "\n",
    "- **Contexto:** Palabras que rodean a una palabra objetivo en un cierto contexto.\n",
    "\n",
    "**Formula Matem√°tica (Skip-gram):**\n",
    "\n",
    "Dado un par (contexto, palabra objetivo) representado como (c, o), la probabilidad de que la palabra objetivo sea o dada la palabra contexto c se puede expresar como:\n",
    "\n",
    "$P(o|c) = \\frac{e^{v_o \\cdot v_c}}{\\sum_{w \\in V} e^{v_w \\cdot v_c}}$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $( v_o )$ es el vector de la palabra objetivo.\n",
    "- $( v_c )$ es el vector de la palabra contexto.\n",
    "- $( \\sum_{w \\in V} )$ es la suma sobre todas las palabras en el vocabulario $( V )$.\n",
    "\n",
    "El objetivo es ajustar los vectores $( v_o )$ y $( v_c )$ de manera que maximicen esta probabilidad.\n",
    "\n",
    "**Ejemplo con Gensim en Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector de la palabra 'word': [-0.01787424  0.00788105  0.17011166]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ejemplo de datos\n",
    "sentences = [\n",
    "    \"Word embeddings represent words as vectors\",\n",
    "    \"Word2Vec is a popular word embedding technique\",\n",
    "    \"It captures semantic relationships between words\",\n",
    "]\n",
    "\n",
    "# Tokenizar las oraciones\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Entrenar el modelo Word2Vec\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=3, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Obtener el vector de una palabra\n",
    "vector_word_embeddings = model.wv['word']\n",
    "\n",
    "# Imprimir el vector de la palabra 'word'\n",
    "print(\"Vector de la palabra 'word':\", vector_word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo utiliza la biblioteca Gensim para entrenar un modelo Word2Vec en un peque√±o conjunto de oraciones. El vector resultante para la palabra 'word' se obtiene mediante la consulta del modelo. El tama√±o del vector se establece en 3 para este ejemplo. En aplicaciones pr√°cticas, los vectores suelen tener dimensiones m√°s altas (por ejemplo, 100, 300)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 47** | **Siguiente 49** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../README.md) | [‚è™](./47_TF-IDF.ipynb)| [‚è©](./49_Poisson_Distribution.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
