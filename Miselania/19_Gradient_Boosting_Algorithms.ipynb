{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atrás 18** | **Siguiente 20** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [🏠](../README.md) | [⏪](./18_Dimensionality_Reduction_Algorithms.ipynb)| [⏩](./20_Bernoulli_Distribution.ipynb)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **19. Gradient Boosting Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de Gradient Boosting son técnicas de aprendizaje automático que construyen un modelo predictivo en forma de un conjunto de modelos más simples, generalmente árboles de decisión débiles. Estos algoritmos se centran en mejorar la precisión del modelo iterativamente mediante la combinación de modelos débiles para formar un modelo más fuerte. XGBoost, LightGBM y CatBoost son ejemplos populares de algoritmos de Gradient Boosting.\n",
    "\n",
    "**Terminología:**\n",
    "\n",
    "- **Árbol Débil:** Un modelo de aprendizaje débil, generalmente un árbol de decisión con una profundidad limitada.\n",
    "- **Residuos:** Las diferencias entre las predicciones actuales y los valores reales.\n",
    "- **Learning Rate (Tasa de Aprendizaje):** Un parámetro que controla la contribución de cada modelo débil al conjunto final.\n",
    "- **Regularización:** Técnicas para evitar el sobreajuste, como la poda de árboles o la limitación de la profundidad.\n",
    "- **Gradient Descent (Descenso de Gradiente):** Un método de optimización que ajusta los parámetros del modelo en la dirección opuesta al gradiente de la función de pérdida.\n",
    "\n",
    "**Fórmula Matemática:**\n",
    "\n",
    "En cada iteración, se ajusta un nuevo modelo débil para corregir los errores del modelo anterior. La predicción final es una combinación ponderada de todas las predicciones débiles. La fórmula general es:\n",
    "\n",
    "$F(x) = F_0(x) + \\eta f_1(x) + \\eta f_2(x) + \\ldots + \\eta f_M(x)$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $F(x)$ es la predicción final.\n",
    "- $F_0(x)$ es el modelo inicial.\n",
    "- $( \\eta )$ es la tasa de aprendizaje.\n",
    "- $f_i(x)$ son los modelos débiles ajustados en cada iteración.\n",
    "\n",
    "**Ejemplo en Python - XGBoost:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cuadrático Medio: 0.22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Cargar el conjunto de datos de viviendas en California\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Crear y entrenar un modelo de regresión con XGBoost\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Error Cuadrático Medio: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo utiliza el conjunto de datos Boston Housing, entrena un modelo XGBoost y evalúa su rendimiento. La función `plot_importance` muestra la importancia de cada característica en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atrás 18** | **Siguiente 20** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [🏠](../README.md) | [⏪](./18_Dimensionality_Reduction_Algorithms.ipynb)| [⏩](./20_Bernoulli_Distribution.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
