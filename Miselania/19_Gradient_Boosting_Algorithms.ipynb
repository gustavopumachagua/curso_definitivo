{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr谩s 18** | **Siguiente 20** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [](../README.md) | [](./18_Dimensionality_Reduction_Algorithms.ipynb)| [](./20_Bernoulli_Distribution.ipynb)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **19. Gradient Boosting Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de Gradient Boosting son t茅cnicas de aprendizaje autom谩tico que construyen un modelo predictivo en forma de un conjunto de modelos m谩s simples, generalmente 谩rboles de decisi贸n d茅biles. Estos algoritmos se centran en mejorar la precisi贸n del modelo iterativamente mediante la combinaci贸n de modelos d茅biles para formar un modelo m谩s fuerte. XGBoost, LightGBM y CatBoost son ejemplos populares de algoritmos de Gradient Boosting.\n",
    "\n",
    "**Terminolog铆a:**\n",
    "\n",
    "- **rbol D茅bil:** Un modelo de aprendizaje d茅bil, generalmente un 谩rbol de decisi贸n con una profundidad limitada.\n",
    "- **Residuos:** Las diferencias entre las predicciones actuales y los valores reales.\n",
    "- **Learning Rate (Tasa de Aprendizaje):** Un par谩metro que controla la contribuci贸n de cada modelo d茅bil al conjunto final.\n",
    "- **Regularizaci贸n:** T茅cnicas para evitar el sobreajuste, como la poda de 谩rboles o la limitaci贸n de la profundidad.\n",
    "- **Gradient Descent (Descenso de Gradiente):** Un m茅todo de optimizaci贸n que ajusta los par谩metros del modelo en la direcci贸n opuesta al gradiente de la funci贸n de p茅rdida.\n",
    "\n",
    "**F贸rmula Matem谩tica:**\n",
    "\n",
    "En cada iteraci贸n, se ajusta un nuevo modelo d茅bil para corregir los errores del modelo anterior. La predicci贸n final es una combinaci贸n ponderada de todas las predicciones d茅biles. La f贸rmula general es:\n",
    "\n",
    "$F(x) = F_0(x) + \\eta f_1(x) + \\eta f_2(x) + \\ldots + \\eta f_M(x)$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $F(x)$ es la predicci贸n final.\n",
    "- $F_0(x)$ es el modelo inicial.\n",
    "- $( \\eta )$ es la tasa de aprendizaje.\n",
    "- $f_i(x)$ son los modelos d茅biles ajustados en cada iteraci贸n.\n",
    "\n",
    "**Ejemplo en Python - XGBoost:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cuadr谩tico Medio: 0.22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Cargar el conjunto de datos de viviendas en California\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Crear y entrenar un modelo de regresi贸n con XGBoost\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadr谩tico medio\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Error Cuadr谩tico Medio: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo utiliza el conjunto de datos Boston Housing, entrena un modelo XGBoost y eval煤a su rendimiento. La funci贸n `plot_importance` muestra la importancia de cada caracter铆stica en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr谩s 18** | **Siguiente 20** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [](../README.md) | [](./18_Dimensionality_Reduction_Algorithms.ipynb)| [](./20_Bernoulli_Distribution.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
